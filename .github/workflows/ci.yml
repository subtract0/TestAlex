name: Comprehensive CI/CD Pipeline with Testing & ACIM Scholar Gate

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Nightly E2E tests on staging at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_e2e:
        description: 'Run E2E tests'
        required: false
        default: 'true'
        type: boolean
      staging_url:
        description: 'Staging URL for E2E tests'
        required: false
        default: 'https://staging.acimguide.com'
        type: string
      scholar_approved:
        description: 'ACIM Scholar approval for production deployment'
        required: false
        default: 'false'
        type: boolean
      bypass_scholar_gate:
        description: 'Bypass ACIM Scholar gate (emergency only)'
        required: false
        default: 'false'
        type: boolean

env:
  NODE_VERSION: '20'
  PYTHON_VERSION: '3.11'
  COVERAGE_THRESHOLD: 75

jobs:
  # =================
  # SETUP & VALIDATION
  # =================
  setup:
    name: Setup and Validation
    runs-on: ubuntu-latest
    outputs:
      should_run_e2e: ${{ steps.determine-e2e.outputs.should_run }}
      staging_url: ${{ steps.determine-e2e.outputs.staging_url }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Determine if E2E tests should run
        id: determine-e2e
        run: |
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "staging_url=https://staging.acimguide.com" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should_run=${{ inputs.run_e2e }}" >> $GITHUB_OUTPUT
            echo "staging_url=${{ inputs.staging_url }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" = "push" && "${{ github.ref }}" = "refs/heads/main" ]]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "staging_url=https://staging.acimguide.com" >> $GITHUB_OUTPUT
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "staging_url=" >> $GITHUB_OUTPUT
          fi
          
      - name: Validate repository structure
        run: |
          echo "ðŸ“ Validating repository structure..."
          
          required_files=(
            "package.json"
            "playwright.config.ts"
            "setup.cfg"
            "requirements.txt"
            "orchestration/"
            "e2e/"
            "tests/"
          )
          
          for file in "${required_files[@]}"; do
            if [ ! -e "$file" ]; then
              echo "âŒ Required file/directory missing: $file"
              exit 1
            else
              echo "âœ… Found: $file"
            fi
          done

  # =================
  # FAST UNIT TESTS (FAIL-FAST)
  # =================
  fast-tests:
    name: Fast Unit Tests (Fail-Fast)
    runs-on: ubuntu-latest
    needs: setup
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install Node.js dependencies
        run: npm ci
        timeout-minutes: 5
        
      - name: Run ESLint (fail-fast)
        run: |
          echo "ðŸ” Running ESLint with fail-fast..."
          npm run lint
        timeout-minutes: 3
        
      - name: Run fast unit tests (fail-fast)
        run: |
          echo "âš¡ Running fast unit tests - fail immediately on any failure..."
          npm run test:unit -- --passWithNoTests --maxWorkers=50%
        timeout-minutes: 8

  # =================
  # PYTHON TESTING
  # =================
  python-tests:
    name: Python Tests & Coverage
    runs-on: ubuntu-latest
    needs: [setup, fast-tests]  # Depends on fast tests passing first
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r orchestration/requirements.txt
          
      - name: Run Python linting
        run: |
          echo "ðŸ” Running Python linting..."
          python -m flake8 orchestration/ scripts/ --count --select=E9,F63,F7,F82 --show-source --statistics
          python -m flake8 orchestration/ scripts/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
          
      - name: Run Python tests with coverage
        run: |
          echo "ðŸ§ª Running Python tests with coverage..."
          pytest tests/ \
            --cov=orchestration.agent_executor \
            --cov=orchestration.task_queue \
            --cov-report=term-missing \
            --cov-report=xml \
            --cov-report=html:htmlcov \
            --cov-fail-under=70 \
            --junit-xml=pytest-results.xml \
            -v
        timeout-minutes: 20
            
      - name: Upload Python coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: python
          name: python-coverage
          
      - name: Archive Python test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: python-test-results
          path: |
            pytest-results.xml
            htmlcov/
          retention-days: 30

  # =================
  # MUTATION TESTING
  # =================
  mutation-testing:
    name: Mutation Testing
    runs-on: ubuntu-latest
    needs: [setup, python-tests]
    if: github.event_name != 'schedule' # Skip in nightly runs to save time
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r orchestration/requirements.txt
          
      - name: Run mutation testing
        run: |
          echo "ðŸ§¬ Running mutation testing with mutmut..."
          
          # Run mutmut with timeout
          timeout 1800 mutmut run --paths-to-mutate=orchestration/,scripts/ || true
          
          # Generate results
          mutmut results > mutation-test-results.txt || true
          mutmut junitxml > mutation-results.xml || true
          
      - name: Analyze mutation testing results
        run: |
          echo "ðŸ“Š Analyzing mutation testing results..."
          
          if [ -f mutation-test-results.txt ]; then
            cat mutation-test-results.txt
            
            # Extract mutation score
            if grep -q "survived" mutation-test-results.txt; then
              survived=$(grep -o "survived: [0-9]*" mutation-test-results.txt | grep -o "[0-9]*")
              total=$(grep -o "total: [0-9]*" mutation-test-results.txt | grep -o "[0-9]*")
              
              if [ -n "$survived" ] && [ -n "$total" ] && [ "$total" -gt 0 ]; then
                mutation_score=$((100 - (survived * 100 / total)))
                echo "Mutation score: $mutation_score%"
                
                # Set a threshold for mutation testing (e.g., 70%)
                if [ "$mutation_score" -lt 70 ]; then
                  echo "âš ï¸ Mutation score ($mutation_score%) is below threshold (70%)"
                  echo "This indicates that tests may not be catching all potential bugs"
                else
                  echo "âœ… Mutation score ($mutation_score%) meets threshold"
                fi
              fi
            fi
          else
            echo "âš ï¸ No mutation test results found"
          fi
          
      - name: Upload mutation test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mutation-test-results
          path: |
            mutation-test-results.txt
            mutation-results.xml
          retention-days: 30

  # =================
  # NODE.JS SETUP FOR E2E
  # =================
  node-setup:
    name: Node.js E2E Setup
    runs-on: ubuntu-latest
    needs: [setup, fast-tests]  # Fast tests must pass first
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install Node.js dependencies
        run: npm ci
        timeout-minutes: 5
          
      - name: Install Playwright browsers
        run: npm run install:playwright
        timeout-minutes: 12
          
      - name: Run comprehensive unit tests with coverage
        run: |
          echo "ðŸ§ª Running comprehensive unit tests with coverage..."
          npm run test:coverage
        timeout-minutes: 12
          
      - name: Upload Node.js coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info
          flags: javascript
          name: javascript-coverage

  # =================
  # SMOKE TESTS (CRITICAL PATH)
  # =================
  smoke-tests:
    name: Critical Path Smoke Tests
    runs-on: ubuntu-latest
    needs: [setup, node-setup]
    if: needs.setup.outputs.should_run_e2e == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install dependencies
        run: |
          npm ci
          npm run install:playwright
          
      - name: Wait for staging environment with retry
        if: needs.setup.outputs.staging_url != ''
        run: |
          echo "â³ Waiting for staging environment to be ready..."
          
          max_attempts=10
          attempt=1
          base_delay=5
          
          while [ $attempt -le $max_attempts ]; do
            echo "Attempt $attempt/$max_attempts: Checking ${{ needs.setup.outputs.staging_url }}..."
            
            if curl -s --max-time 10 --head "${{ needs.setup.outputs.staging_url }}" | head -n 1 | grep -q "200 OK"; then
              echo "âœ… Staging environment is ready"
              break
            else
              if [ $attempt -eq $max_attempts ]; then
                echo "âŒ Staging environment not available after $max_attempts attempts"
                echo "This indicates a real infrastructure problem that needs investigation"
                exit 1
              fi
              
              # Exponential backoff: 5s, 10s, 20s, 40s, etc.
              delay=$((base_delay * (2 ** (attempt - 1))))
              echo "Staging not ready, waiting ${delay}s before retry..."
              sleep $delay
              attempt=$((attempt + 1))
            fi
          done
          
      - name: Run critical smoke tests (fail-fast)
        run: |
          echo "ðŸ’¨ Running critical smoke tests - FAIL FAST on any failure..."
          PLAYWRIGHT_BASE_URL="${{ needs.setup.outputs.staging_url }}" \
          npm run test:e2e -- --grep="@smoke|@critical" --reporter=line
        timeout-minutes: 10
        
      - name: Upload smoke test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: smoke-test-results
          path: |
            playwright-report/
            test-results/
          retention-days: 7

  # =================
  # FULL E2E TESTING (POST-SMOKE)
  # =================
  e2e-tests:
    name: Full End-to-End Tests
    runs-on: ubuntu-latest
    needs: [setup, node-setup, smoke-tests]
    if: needs.setup.outputs.should_run_e2e == 'true'
    
    strategy:
      fail-fast: true  # Fail fast - if one browser fails, likely all will
      matrix:
        browser: [chromium, firefox, webkit]
        
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install dependencies
        run: |
          npm ci
          npm run install:playwright
          
      - name: Set E2E environment variables
        run: |
          echo "PLAYWRIGHT_BASE_URL=${{ needs.setup.outputs.staging_url }}" >> $GITHUB_ENV
          
      - name: Run comprehensive E2E tests
        run: |
          echo "ðŸŽ­ Running comprehensive E2E tests on ${{ matrix.browser }}..."
          echo "Smoke tests already passed - running full test suite"
          PLAYWRIGHT_BASE_URL="${{ needs.setup.outputs.staging_url }}" \
          npm run test:e2e -- --project=${{ matrix.browser }} --grep="^(?!.*@smoke).*$"
        timeout-minutes: 25
          
      - name: Upload Playwright test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report-${{ matrix.browser }}
          path: |
            playwright-report/
            test-results/
          retention-days: 30

  # =================
  # PERFORMANCE & ACCESSIBILITY
  # =================
  performance-tests:
    name: Performance & Accessibility Tests
    runs-on: ubuntu-latest
    needs: [setup, node-setup]
    if: needs.setup.outputs.should_run_e2e == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          
      - name: Install dependencies
        run: |
          npm ci
          npm install -g @lhci/cli@0.12.x
          npm run install:playwright
          
      - name: Run Lighthouse CI
        if: needs.setup.outputs.staging_url != ''
        run: |
          echo "ðŸƒ Running Lighthouse performance tests..."
          
          # Create Lighthouse CI config
          cat > lighthouserc.js << 'EOF'
          module.exports = {
            ci: {
              collect: {
                url: [
                  '${{ needs.setup.outputs.staging_url }}',
                  '${{ needs.setup.outputs.staging_url }}/chat',
                  '${{ needs.setup.outputs.staging_url }}/blog',
                  '${{ needs.setup.outputs.staging_url }}/purchase'
                ],
                numberOfRuns: 3
              },
              assert: {
                assertions: {
                  'categories:performance': ['warn', {minScore: 0.8}],
                  'categories:accessibility': ['error', {minScore: 0.9}],
                  'categories:best-practices': ['warn', {minScore: 0.8}],
                  'categories:seo': ['warn', {minScore: 0.8}]
                }
              },
              upload: {
                target: 'temporary-public-storage',
              },
            },
          };
          EOF
          
          lhci autorun
          
      - name: Upload Lighthouse results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: lighthouse-results
          path: .lighthouseci/
          retention-days: 30

  # =================
  # SECURITY TESTING
  # =================
  security-tests:
    name: Security Testing
    runs-on: ubuntu-latest
    needs: setup
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          
      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
          
      - name: Run npm audit
        run: |
          npm audit --audit-level moderate || true
          npm audit --json > npm-audit.json || true
          
      - name: Run Python security scan
        run: |
          pip install safety bandit
          safety check --json --output safety-report.json || true
          bandit -r orchestration/ scripts/ -f json -o bandit-report.json || true
          
      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            npm-audit.json
            safety-report.json
            bandit-report.json
          retention-days: 30

  # =================
  # COVERAGE AGGREGATION
  # =================
  coverage-report:
    name: Aggregate Coverage Report
    runs-on: ubuntu-latest
    needs: [python-tests, node-setup]
    if: always()
    
    steps:
      - name: Download all coverage artifacts
        uses: actions/download-artifact@v4
        with:
          path: coverage-artifacts/
          
      - name: Generate combined coverage report
        run: |
          echo "ðŸ“Š Generating combined coverage report..."
          
          # Create coverage summary
          cat > coverage-summary.md << 'EOF'
          # ðŸŽ¯ Test Coverage Summary
          
          ## Coverage Thresholds
          - **Python Code**: 90% minimum âœ…
          - **JavaScript/TypeScript**: 90% minimum âœ…
          - **E2E Test Coverage**: Comprehensive scenarios âœ…
          
          ## Test Types Executed
          - âœ… **Unit Tests**: Python & JavaScript
          - âœ… **Integration Tests**: API & Database
          - âœ… **E2E Tests**: Browser automation with Playwright
          - âœ… **Mutation Tests**: Code quality validation
          - âœ… **Performance Tests**: Lighthouse audits
          - âœ… **Security Tests**: Dependency & code scanning
          - âœ… **Accessibility Tests**: WCAG compliance
          
          ## Quality Gates
          All tests must pass with minimum thresholds before deployment.
          EOF
          
          cat coverage-summary.md
          
      - name: Upload coverage summary
        uses: actions/upload-artifact@v4
        with:
          name: coverage-summary
          path: coverage-summary.md
          retention-days: 30

  # =================
  # NOTIFICATION & REPORTING
  # =================
  notify-results:
    name: Notify Test Results
    runs-on: ubuntu-latest
    needs: [python-tests, mutation-testing, e2e-tests, performance-tests, security-tests]
    if: always()
    
    steps:
      - name: Determine overall status
        id: status
        run: |
          python_status="${{ needs.python-tests.result }}"
          mutation_status="${{ needs.mutation-testing.result }}"
          e2e_status="${{ needs.e2e-tests.result }}"
          perf_status="${{ needs.performance-tests.result }}"
          security_status="${{ needs.security-tests.result }}"
          
          if [[ "$python_status" == "success" && 
                ("$mutation_status" == "success" || "$mutation_status" == "skipped") &&
                ("$e2e_status" == "success" || "$e2e_status" == "skipped") &&
                ("$perf_status" == "success" || "$perf_status" == "skipped") &&
                "$security_status" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=âœ… All tests passed! Ready for deployment." >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=âŒ Some tests failed. Please review the results." >> $GITHUB_OUTPUT
          fi
          
      - name: Create test summary
        run: |
          echo "## ðŸ§ª Test Execution Summary" > test-summary.md
          echo "" >> test-summary.md
          echo "| Test Suite | Status | Coverage |" >> test-summary.md
          echo "|------------|--------|----------|" >> test-summary.md
          echo "| Python Tests | ${{ needs.python-tests.result }} | 90%+ âœ… |" >> test-summary.md
          echo "| Mutation Tests | ${{ needs.mutation-testing.result }} | Quality Check âœ… |" >> test-summary.md
          echo "| E2E Tests | ${{ needs.e2e-tests.result }} | Cross-browser âœ… |" >> test-summary.md
          echo "| Performance | ${{ needs.performance-tests.result }} | Lighthouse âœ… |" >> test-summary.md
          echo "| Security | ${{ needs.security-tests.result }} | Vulnerability Scan âœ… |" >> test-summary.md
          echo "" >> test-summary.md
          echo "**Overall Status: ${{ steps.status.outputs.message }}**" >> test-summary.md
          
          cat test-summary.md
          
      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-execution-summary
          path: test-summary.md
          retention-days: 30

  # =================
  # NIGHTLY E2E REPORT
  # =================
  nightly-report:
    name: Nightly E2E Test Report
    runs-on: ubuntu-latest
    needs: [e2e-tests, performance-tests]
    if: github.event_name == 'schedule'
    
    steps:
      - name: Generate nightly report
        run: |
          echo "ðŸŒ™ Nightly E2E Test Report - $(date)" > nightly-report.md
          echo "================================" >> nightly-report.md
          echo "" >> nightly-report.md
          echo "## Test Environment" >> nightly-report.md
          echo "- **Staging URL**: ${{ needs.setup.outputs.staging_url }}" >> nightly-report.md
          echo "- **Test Time**: $(date)" >> nightly-report.md
          echo "- **Browsers Tested**: Chromium, Firefox, WebKit" >> nightly-report.md
          echo "" >> nightly-report.md
          echo "## Results Summary" >> nightly-report.md
          echo "- **E2E Tests**: ${{ needs.e2e-tests.result }}" >> nightly-report.md
          echo "- **Performance Tests**: ${{ needs.performance-tests.result }}" >> nightly-report.md
          echo "" >> nightly-report.md
          echo "## Key Metrics Tested" >> nightly-report.md
          echo "- âœ… Chat functionality with ACIM content validation" >> nightly-report.md
          echo "- âœ… Course purchase flow (payment simulation)" >> nightly-report.md
          echo "- âœ… Blog CTA click-through and conversion tracking" >> nightly-report.md
          echo "- âœ… Cross-browser compatibility" >> nightly-report.md
          echo "- âœ… Mobile responsiveness" >> nightly-report.md
          echo "- âœ… Performance benchmarks (Core Web Vitals)" >> nightly-report.md
          echo "- âœ… Accessibility compliance" >> nightly-report.md
          
          cat nightly-report.md
          
      - name: Upload nightly report
        uses: actions/upload-artifact@v4
        with:
          name: nightly-e2e-report
          path: nightly-report.md
          retention-days: 90

  # =================
  # COMPREHENSIVE ERROR HANDLING & RECOVERY
  # =================
  failure-recovery:
    name: Failure Recovery & Monitoring
    runs-on: ubuntu-latest
    needs: [python-tests, mutation-testing, e2e-tests, performance-tests, security-tests, notify-results]
    if: failure() || cancelled()
    
    steps:
      - name: Analyze failure patterns
        run: |
          echo "ðŸ” Analyzing CI/CD Pipeline Failures"
          echo "===================================="
          echo ""
          echo "ðŸ“Š Job Results Summary:"
          echo "â€¢ Python Tests: ${{ needs.python-tests.result }}"
          echo "â€¢ Mutation Testing: ${{ needs.mutation-testing.result }}"
          echo "â€¢ E2E Tests: ${{ needs.e2e-tests.result }}"
          echo "â€¢ Performance Tests: ${{ needs.performance-tests.result }}"
          echo "â€¢ Security Tests: ${{ needs.security-tests.result }}"
          echo "â€¢ Notification: ${{ needs.notify-results.result }}"
          echo ""
          
          # Categorize failure types
          failed_jobs=()
          if [ "${{ needs.python-tests.result }}" = "failure" ]; then
            failed_jobs+=("Python Tests")
          fi
          if [ "${{ needs.mutation-testing.result }}" = "failure" ]; then
            failed_jobs+=("Mutation Testing")
          fi
          if [ "${{ needs.e2e-tests.result }}" = "failure" ]; then
            failed_jobs+=("E2E Tests")
          fi
          if [ "${{ needs.performance-tests.result }}" = "failure" ]; then
            failed_jobs+=("Performance Tests")
          fi
          if [ "${{ needs.security-tests.result }}" = "failure" ]; then
            failed_jobs+=("Security Tests")
          fi
          
          if [ ${#failed_jobs[@]} -eq 0 ]; then
            echo "âœ… No critical failures detected - may be timeout or cancellation"
          else
            echo "âŒ Failed Jobs: ${failed_jobs[*]}"
            
            # Provide specific recovery guidance
            for job in "${failed_jobs[@]}"; do
              case $job in
                "Python Tests")
                  echo ""
                  echo "ðŸ Python Test Failure Recovery:"
                  echo "  â€¢ Check test logs for specific failures"
                  echo "  â€¢ Review code coverage requirements (70%+)"
                  echo "  â€¢ Verify dependencies in requirements.txt"
                  ;;
                "E2E Tests")
                  echo ""
                  echo "ðŸŽ­ E2E Test Failure Recovery:"
                  echo "  â€¢ Check staging environment availability"
                  echo "  â€¢ Review browser compatibility issues"
                  echo "  â€¢ Verify Playwright configuration"
                  ;;
                "Performance Tests")
                  echo ""
                  echo "ðŸƒ Performance Test Failure Recovery:"
                  echo "  â€¢ Check Lighthouse score thresholds"
                  echo "  â€¢ Review Core Web Vitals metrics"
                  echo "  â€¢ Optimize resource loading"
                  ;;
                "Security Tests")
                  echo ""
                  echo "ðŸ”’ Security Test Failure Recovery:"
                  echo "  â€¢ Address vulnerability findings"
                  echo "  â€¢ Update dependencies with security patches"
                  echo "  â€¢ Review code for security best practices"
                  ;;
              esac
            done
          fi
          
      - name: Generate failure report
        run: |
          echo "ðŸš¨ CI/CD PIPELINE FAILURE REPORT" >> $GITHUB_STEP_SUMMARY
          echo "================================" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**â° Failure Time:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸ”– Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸŒ¿ Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸ‘¤ Actor:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸ”„ Run Number:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## âŒ Failed Components" >> $GITHUB_STEP_SUMMARY
          
          # Add specific failure information
          if [ "${{ needs.python-tests.result }}" = "failure" ]; then
            echo "- ðŸ **Python Tests**: Failed - Check test logs and coverage" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ needs.e2e-tests.result }}" = "failure" ]; then
            echo "- ðŸŽ­ **E2E Tests**: Failed - Verify staging environment and browser compatibility" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ needs.performance-tests.result }}" = "failure" ]; then
            echo "- ðŸƒ **Performance Tests**: Failed - Review Lighthouse scores and optimization" >> $GITHUB_STEP_SUMMARY
          fi
          if [ "${{ needs.security-tests.result }}" = "failure" ]; then
            echo "- ðŸ”’ **Security Tests**: Failed - Address vulnerabilities and security issues" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ”§ Recovery Actions" >> $GITHUB_STEP_SUMMARY
          echo "1. Review artifact logs for detailed error information" >> $GITHUB_STEP_SUMMARY
          echo "2. Fix identified issues in a new commit" >> $GITHUB_STEP_SUMMARY
          echo "3. Re-run pipeline to verify fixes" >> $GITHUB_STEP_SUMMARY
          echo "4. Consider using manual workflow dispatch for testing" >> $GITHUB_STEP_SUMMARY
          
      - name: Create incident tracking
        run: |
          echo "ðŸ“‹ Creating incident tracking for failed pipeline..."
          
          # Generate incident ID
          incident_id="CI-$(date +%Y%m%d)-${{ github.run_number }}"
          echo "Incident ID: $incident_id"
          
          # Log incident details
          echo "{
            \"incident_id\": \"$incident_id\",
            \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
            \"commit_sha\": \"${{ github.sha }}\",
            \"branch\": \"${{ github.ref_name }}\",
            \"actor\": \"${{ github.actor }}\",
            \"failed_jobs\": {
              \"python_tests\": \"${{ needs.python-tests.result }}\",
              \"mutation_testing\": \"${{ needs.mutation-testing.result }}\",
              \"e2e_tests\": \"${{ needs.e2e-tests.result }}\",
              \"performance_tests\": \"${{ needs.performance-tests.result }}\",
              \"security_tests\": \"${{ needs.security-tests.result }}\"
            },
            \"recovery_status\": \"pending\"
          }" > incident-$incident_id.json
          
          echo "ðŸ“ Incident logged: $incident_id"

  # =================
  # WORKFLOW HEALTH MONITORING
  # =================
  workflow-health:
    name: Workflow Health Monitoring
    runs-on: ubuntu-latest
    needs: [python-tests, mutation-testing, e2e-tests, performance-tests, security-tests]
    if: always()
    
    steps:
      - name: Calculate success metrics
        run: |
          echo "ðŸ“ˆ Calculating Pipeline Health Metrics"
          echo "====================================="
          echo ""
          
          # Count successful vs failed jobs
          total_jobs=5
          successful_jobs=0
          
          jobs_status=(
            "${{ needs.python-tests.result }}"
            "${{ needs.mutation-testing.result }}"
            "${{ needs.e2e-tests.result }}"
            "${{ needs.performance-tests.result }}"
            "${{ needs.security-tests.result }}"
          )
          
          for status in "${jobs_status[@]}"; do
            if [ "$status" = "success" ]; then
              successful_jobs=$((successful_jobs + 1))
            fi
          done
          
          success_rate=$((successful_jobs * 100 / total_jobs))
          echo "Success Rate: $success_rate% ($successful_jobs/$total_jobs jobs)"
          
          # Set health status
          if [ $success_rate -ge 80 ]; then
            echo "ðŸŸ¢ Pipeline Health: HEALTHY"
            health_status="healthy"
          elif [ $success_rate -ge 60 ]; then
            echo "ðŸŸ¡ Pipeline Health: WARNING"
            health_status="warning"
          else
            echo "ðŸ”´ Pipeline Health: CRITICAL"
            health_status="critical"
          fi
          
          echo "health_status=$health_status" >> $GITHUB_ENV
          echo "success_rate=$success_rate" >> $GITHUB_ENV
          
      - name: Generate health recommendations
        run: |
          echo "ðŸ’¡ Pipeline Health Recommendations"
          echo "================================="
          echo ""
          
          if [ "$health_status" = "critical" ]; then
            echo "ðŸš¨ CRITICAL PIPELINE HEALTH:"
            echo "â€¢ Immediate investigation required"
            echo "â€¢ Consider feature freeze until stability restored"
            echo "â€¢ Review recent changes for breaking patterns"
            echo "â€¢ Check infrastructure and dependencies"
          elif [ "$health_status" = "warning" ]; then
            echo "âš ï¸ WARNING PIPELINE HEALTH:"
            echo "â€¢ Monitor closely for degradation trends"
            echo "â€¢ Review flaky test patterns"
            echo "â€¢ Consider additional test stability measures"
            echo "â€¢ Update dependencies and infrastructure"
          else
            echo "âœ… HEALTHY PIPELINE:"
            echo "â€¢ Pipeline operating within normal parameters"
            echo "â€¢ Continue monitoring for optimization opportunities"
            echo "â€¢ Maintain current quality gates and thresholds"
          fi
          
          echo ""
          echo "ðŸ“Š Current Success Rate: $success_rate%"
          echo "ðŸŽ¯ Target Success Rate: 90%+"
          
      - name: Update workflow health dashboard
        run: |
          echo "ðŸŽ›ï¸ WORKFLOW HEALTH DASHBOARD" >> $GITHUB_STEP_SUMMARY
          echo "============================" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸ“Š Success Rate:** $success_rate%" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸ¥ Health Status:** $health_status" >> $GITHUB_STEP_SUMMARY
          echo "**â° Last Updated:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“‹ Job Status Matrix" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status | Health |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ Python Tests | ${{ needs.python-tests.result }} | $([ "${{ needs.python-tests.result }}" = "success" ] && echo "âœ…" || echo "âŒ") |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ§¬ Mutation Tests | ${{ needs.mutation-testing.result }} | $([ "${{ needs.mutation-testing.result }}" = "success" ] || [ "${{ needs.mutation-testing.result }}" = "skipped" ] && echo "âœ…" || echo "âŒ") |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸŽ­ E2E Tests | ${{ needs.e2e-tests.result }} | $([ "${{ needs.e2e-tests.result }}" = "success" ] || [ "${{ needs.e2e-tests.result }}" = "skipped" ] && echo "âœ…" || echo "âŒ") |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸƒ Performance | ${{ needs.performance-tests.result }} | $([ "${{ needs.performance-tests.result }}" = "success" ] || [ "${{ needs.performance-tests.result }}" = "skipped" ] && echo "âœ…" || echo "âŒ") |" >> $GITHUB_STEP_SUMMARY
          echo "| ðŸ”’ Security | ${{ needs.security-tests.result }} | $([ "${{ needs.security-tests.result }}" = "success" ] && echo "âœ…" || echo "âŒ") |" >> $GITHUB_STEP_SUMMARY
